"""
LOCAL MODEL TESTING SCRIPT
Tests each detection layer independently to verify:
1. Weights load correctly
2. Models run inference successfully
3. Output format matches expectations

Run this BEFORE deploying to Modal
"""

import torch
import numpy as np
import sys
from pathlib import Path
import time

# Add repo paths
sys.path.insert(0, str(Path(__file__).parent / "LipForensics"))
sys.path.insert(0, str(Path(__file__).parent / "UniversalFakeDetect"))

print("="*80)
print("DEEPFAKE DETECTION MODEL VALIDATION SUITE")
print("="*80)

# ============================================================================
# TEST 1: AUDIO LAYER (Wav2Vec2 from HuggingFace)
# ============================================================================

def test_audio_model():
    print("\n[TEST 1/4] Audio Detection (Wav2Vec2-XLS-R)")
    print("-" * 80)
    
    try:
        from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor
        import librosa
        
        # Use local model directory
        model_path = Path(__file__).parent / "Deepfake-audio-detection-V2"
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        print(f"Loading model from local path: {model_path}")
        print(f"Device: {device}")
        
        # For audio classification, we only need the feature extractor (not tokenizer)
        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(str(model_path))
        model = Wav2Vec2ForSequenceClassification.from_pretrained(str(model_path)).to(device)
        model.eval()
        
        print(f"‚úì Model loaded successfully")
        print(f"  - Parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"  - GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB" if torch.cuda.is_available() else "  - Running on CPU")
        
        # Test inference with dummy audio (1 second of silence at 16kHz)
        dummy_audio = np.zeros(16000, dtype=np.float32)
        
        inputs = feature_extractor(
            dummy_audio,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True
        ).to(device)
        
        print(f"\nRunning test inference...")
        start_time = time.time()
        
        with torch.no_grad():
            logits = model(**inputs).logits
            probs = torch.softmax(logits, dim=-1)
        
        inference_time = time.time() - start_time
        
        print(f"‚úì Inference successful")
        print(f"  - Output shape: {logits.shape}")
        print(f"  - Probabilities: {probs[0].cpu().numpy()}")
        print(f"  - Inference time: {inference_time*1000:.1f}ms")
        print(f"\n‚úÖ LAYER 1 (AUDIO): PASSED")
        
        # Cleanup
        del model, feature_extractor
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå LAYER 1 (AUDIO): FAILED")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# TEST 2: VISUAL LAYER (EfficientNet-B4 SBI)
# ============================================================================

def test_visual_model():
    print("\n[TEST 2/4] Visual Artifacts Detection (EfficientNet-B4 SBI)")
    print("-" * 80)
    
    try:
        from efficientnet_pytorch import EfficientNet
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        weights_path = Path(__file__).parent / "weights" / "SBI" / "FFc23.tar"
        
        print(f"Loading weights from: {weights_path}")
        print(f"Note: PyTorch checkpoint in ZIP format")
        
        # Load model
        print(f"Loading EfficientNet-B4...")
        model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=1).to(device)
        
        # Load custom weights directly (PyTorch can read ZIP-format checkpoints)
        print(f"Loading checkpoint...")
        checkpoint = torch.load(weights_path, map_location=device, weights_only=False)
        
        print(f"Checkpoint keys: {list(checkpoint.keys())[:5] if hasattr(checkpoint, 'keys') else 'Direct state dict'}")
        
        # Try different possible state dict locations
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
            print(f"‚úì Loaded checkpoint with model_state_dict key")
        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'], strict=False)
            print(f"‚úì Loaded checkpoint with state_dict key")
        elif isinstance(checkpoint, dict) and 'model' in checkpoint:
            model.load_state_dict(checkpoint['model'], strict=False)
            print(f"‚úì Loaded checkpoint with model key")
        else:
            # Checkpoint is direct state dict
            model.load_state_dict(checkpoint, strict=False)
            print(f"‚úì Loaded checkpoint (direct state dict)")
        
        model.eval()
        
        print(f"‚úì Model loaded successfully")
        print(f"  - Parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"  - GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB" if torch.cuda.is_available() else "  - Running on CPU")
        
        # Test inference with dummy image (380x380x3)
        dummy_image = torch.randn(1, 3, 380, 380).to(device)
        
        print(f"\nRunning test inference...")
        start_time = time.time()
        
        with torch.no_grad():
            output = model(dummy_image)
            prob = torch.sigmoid(output)
        
        inference_time = time.time() - start_time
        
        print(f"‚úì Inference successful")
        print(f"  - Output shape: {output.shape}")
        print(f"  - Probability: {prob.item():.4f}")
        print(f"  - Inference time: {inference_time*1000:.1f}ms")
        print(f"\n‚úÖ LAYER 2 (VISUAL): PASSED")
        
        # Cleanup
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå LAYER 2 (VISUAL): FAILED")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# TEST 3: LIP-SYNC LAYER (LipForensics)
# ============================================================================

def test_lipsync_model():
    print("\n[TEST 3/4] Lip-Sync Detection (LipForensics)")
    print("-" * 80)
    
    try:
        weights_path = Path(__file__).parent / "weights" / "Lips" / "lipforensics_ff.pth"
        
        print(f"Checking weights: {weights_path}")
        
        if not weights_path.exists():
            raise FileNotFoundError(f"LipForensics weights not found at {weights_path}")
        
        print(f"‚úì Weights file exists ({weights_path.stat().st_size / 1e6:.1f}MB)")
        
        # Try to load checkpoint to verify format
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        checkpoint = torch.load(weights_path, map_location=device)
        
        print(f"‚úì Checkpoint loaded successfully")
        print(f"  - Keys: {list(checkpoint.keys())[:5]}...")
        
        # Note: Full LipForensics requires the model architecture code
        # This test only verifies the weights can be loaded
        print(f"\n‚ö†Ô∏è  LAYER 3 (LIP-SYNC): PARTIAL CHECK")
        print(f"  - Weights file is valid")
        print(f"  - Full model test requires LipForensics architecture implementation")
        print(f"  - This is expected for Phase 1 (placeholder ready)")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå LAYER 3 (LIP-SYNC): FAILED")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# TEST 4: SEMANTIC LAYER (UniversalFakeDetect - CLIP)
# ============================================================================

def test_semantic_model():
    print("\n[TEST 4/4] Generative Semantic Detection (CLIP + UniversalFakeDetect)")
    print("-" * 80)
    
    try:
        import clip
        import torch.nn as nn
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        fc_weights_path = Path(__file__).parent / "UniversalFakeDetect" / "pretrained_weights" / "fc_weights.pth"
        
        print(f"Checking FC weights: {fc_weights_path}")
        
        if not fc_weights_path.exists():
            raise FileNotFoundError(f"UniversalFakeDetect FC weights not found at {fc_weights_path}")
        
        print(f"‚úì FC weights exist ({fc_weights_path.stat().st_size / 1e6:.1f}MB)")
        
        # Load CLIP
        print(f"Loading CLIP ViT-L/14...")
        clip_model, preprocess = clip.load("ViT-L/14", device=device)
        clip_model.eval()
        
        print(f"‚úì CLIP model loaded successfully")
        print(f"  - Parameters: {sum(p.numel() for p in clip_model.parameters()):,}")
        
        # Load custom FC layer
        print(f"Loading custom FC layer...")
        fc_layer = nn.Linear(768, 1).to(device)
        
        fc_checkpoint = torch.load(fc_weights_path, map_location=device)
        
        # Handle different checkpoint formats
        if isinstance(fc_checkpoint, dict):
            if 'model_state_dict' in fc_checkpoint:
                fc_layer.load_state_dict(fc_checkpoint['model_state_dict'], strict=False)
            elif 'state_dict' in fc_checkpoint:
                fc_layer.load_state_dict(fc_checkpoint['state_dict'], strict=False)
            else:
                fc_layer.load_state_dict(fc_checkpoint, strict=False)
        else:
            # Checkpoint is raw state dict
            fc_layer.load_state_dict(fc_checkpoint, strict=False)
        
        fc_layer.eval()
        
        print(f"‚úì FC layer loaded successfully")
        print(f"  - GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB" if torch.cuda.is_available() else "  - Running on CPU")
        
        # Test inference with dummy image (224x224x3 for CLIP)
        dummy_image = torch.randn(1, 3, 224, 224).to(device)
        
        print(f"\nRunning test inference...")
        start_time = time.time()
        
        with torch.no_grad():
            features = clip_model.encode_image(dummy_image)
            features = features / features.norm(dim=-1, keepdim=True)
            logit = fc_layer(features.float())
            prob = torch.sigmoid(logit)
        
        inference_time = time.time() - start_time
        
        print(f"‚úì Inference successful")
        print(f"  - Feature shape: {features.shape}")
        print(f"  - Logit: {logit.item():.4f}")
        print(f"  - Probability: {prob.item():.4f}")
        print(f"  - Inference time: {inference_time*1000:.1f}ms")
        print(f"\n‚úÖ LAYER 4 (SEMANTIC): PASSED")
        
        # Cleanup
        del clip_model, fc_layer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå LAYER 4 (SEMANTIC): FAILED")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

if __name__ == "__main__":
    print(f"\nSystem Information:")
    print(f"  - PyTorch: {torch.__version__}")
    print(f"  - CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPU: {torch.cuda.get_device_name(0)}")
        print(f"  - CUDA Version: {torch.version.cuda}")
        print(f"  - Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    results = {}
    
    # Run all tests
    results['audio'] = test_audio_model()
    results['visual'] = test_visual_model()
    results['lipsync'] = test_lipsync_model()
    results['semantic'] = test_semantic_model()
    
    # Summary
    print("\n" + "="*80)
    print("VALIDATION SUMMARY")
    print("="*80)
    
    for layer, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{layer.upper():<15} {status}")
    
    total_passed = sum(results.values())
    total_tests = len(results)
    
    print(f"\nTotal: {total_passed}/{total_tests} tests passed")
    
    if total_passed == total_tests:
        print("\nüéâ ALL MODELS VALIDATED - Ready for Modal deployment!")
    else:
        print("\n‚ö†Ô∏è  Some models failed - Fix errors before deploying")
    
    print("="*80)
